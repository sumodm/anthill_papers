### Anthill Inside 2018 Roadmap Papers (Please add your suggestions here) 
##### 1 DL: Overview: [Nature: Deep Learning, LeCun et al.](http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf)
Good survey paper to get introduction to Deep Learning. Provides the history and overview of how Deep Learning evolved. They provide an introduction to supervised learning and then provides with biological motivations behind these architectures. They also explain how the CNN and RNN architetures work and showcases some successful applications.

##### 2 DL: Arch: [Alexnet: Imagenet classification with deep convolutional neural networks](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)
This landmark paper in CNN architecture showcased remarkable results for the 2012 ILSVRC Challenge and ahead of their competition by about 10.8%. Their main contributions are that they used Relu instead of Tanh and dropout to handle overfitting.

##### 3 DL: Arch: [VGG: Very deep convolutional networks for large-scale image recognition: Simonyan et al](https://arxiv.org/pdf/1409.1556.pdf)
This paper was runner up of 2014 ILSVRC Challenge. The architecture is very uniform (composed of few modules, each modules being few convolution layers with a pooling layer at the end). This particular model has large number of parameters and therefore more challenging to handle. However this is very appealing for feature extraction.

##### 4 DL: Arch: [GoogLeNet: Going Deeper with Convolutions: Szegedy et al](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf)
Winner of 2014 ILSVRC Challenge achieving 6.67% top-5 accuracy. The main contribution is the inception module which allows to reduce the parameters drastically. This allows for creating large networks and the winning network had 22 layers.

##### 5 DL: Arch: [Deep Residual Learning for Image Recognition: He et al](https://arxiv.org/pdf/1512.03385.pdf)
This paper appeaard in 2015 ILSVRC challenge. Their main contribution was skip connections that allowed gradients to bypass certain layers and connect to layers much higher or lower in the networks. This allowed them to create very large networks, as big as 152 layers large.

-----------------------------------------------------

### Anthill Inside 2018 Papers Discussion Session
#### Classic Paper Suggestions (Please add your suggestions here)
##### 1. 
##### 2. 

#### Recent Innovative Paper Suggestions (Please add your suggestions here)
##### 1. 
##### 2. 
